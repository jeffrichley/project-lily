# CLI Testing Guidelines

## Overview
This document outlines testing strategies and best practices for the Lily CLI application.

## Test Structure

### Unit Tests
- Test individual CLI commands in isolation
- Mock external dependencies (file system, network calls)
- Use `pytest` fixtures for common setup
- Test both success and failure scenarios

### Integration Tests
- Test command interactions with real file system
- Test end-to-end workflows
- Use temporary directories for file operations
- Test error handling and edge cases

### Example Test Structure

```python
import pytest
from typer.testing import CliRunner
from lily.cli.main import app

@pytest.fixture
def runner():
    return CliRunner()

def test_start_command_success(runner):
    """Test start command with valid input."""
    # Arrange
    # Act
    result = runner.invoke(app, ["start"])
    # Assert
    assert result.exit_code == 0
    assert "Starting Lily" in result.stdout

def test_start_command_with_config(runner, tmp_path):
    """Test start command with custom config."""
    # Arrange
    config_file = tmp_path / "config.toml"
    config_file.write_text("openai_api_key = 'test-key'")
    
    # Act
    result = runner.invoke(app, ["start", "--config", str(config_file)])
    
    # Assert
    assert result.exit_code == 0
```

## Testing Best Practices

### 1. Use Temporary Directories
```python
def test_file_operations(runner, tmp_path):
    """Test file operations with temporary directory."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("test content")
    
    result = runner.invoke(app, ["process", str(test_file)])
    assert result.exit_code == 0
```

### 2. Mock External Dependencies
```python
@patch('lily.config.openai_api_call')
def test_openai_integration(mock_api, runner):
    """Test OpenAI integration with mocked API."""
    mock_api.return_value = "Mocked response"
    
    result = runner.invoke(app, ["ask", "test question"])
    assert result.exit_code == 0
    assert "Mocked response" in result.stdout
```

### 3. Test Error Scenarios
```python
def test_invalid_config(runner, tmp_path):
    """Test handling of invalid configuration."""
    config_file = tmp_path / "invalid.toml"
    config_file.write_text("invalid toml content [")
    
    result = runner.invoke(app, ["start", "--config", str(config_file)])
    assert result.exit_code != 0
    assert "error" in result.stderr.lower()
```

### 4. Test Interactive Commands
```python
def test_interactive_command(runner):
    """Test interactive command execution."""
    # Use input_stream for interactive testing
    result = runner.invoke(
        app, 
        ["interactive"], 
        input="help\nexit\n"
    )
    assert result.exit_code == 0
```

## Coverage Requirements

### Minimum Coverage
- **Unit Tests**: 80% line coverage
- **Integration Tests**: 70% line coverage
- **Critical Paths**: 100% coverage

### Coverage Exclusions
- Error handling for unrecoverable errors
- Platform-specific code paths
- Debug/development-only code

## Test Data Management

### Fixtures
```python
@pytest.fixture
def sample_config():
    """Provide sample configuration for testing."""
    return {
        "openai_api_key": "test-key",
        "model": "gpt-4",
        "theme": "light"
    }

@pytest.fixture
def temp_config_file(sample_config, tmp_path):
    """Create temporary config file."""
    config_file = tmp_path / "config.toml"
    with open(config_file, "w") as f:
        tomli_w.dump(sample_config, f)
    return config_file
```

### Test Data Files
- Store test data in `tests/data/` directory
- Use descriptive file names
- Include both valid and invalid test cases

## Performance Testing

### Command Response Time
```python
import time

def test_command_performance(runner):
    """Test command response time."""
    start_time = time.time()
    result = runner.invoke(app, ["version"])
    end_time = time.time()
    
    assert result.exit_code == 0
    assert (end_time - start_time) < 1.0  # Should complete within 1 second
```

### Memory Usage
```python
import psutil
import os

def test_memory_usage(runner):
    """Test memory usage during command execution."""
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss
    
    result = runner.invoke(app, ["start"])
    
    final_memory = process.memory_info().rss
    memory_increase = final_memory - initial_memory
    
    assert memory_increase < 50 * 1024 * 1024  # Less than 50MB increase
```

## Continuous Integration

### GitHub Actions
```yaml
- name: Run Tests
  run: |
    just test
    just lint
    just type-check
```

### Pre-commit Hooks
```yaml
- repo: local
  hooks:
    - id: pytest
      name: pytest
      entry: pytest
      language: system
      pass_filenames: false
      always_run: true
```

## Debugging Tests

### Verbose Output
```bash
pytest -v --tb=short
```

### Debug Mode
```python
def test_debug_example(runner):
    """Example with debug output."""
    result = runner.invoke(app, ["start"], catch_exceptions=False)
    
    if result.exit_code != 0:
        print(f"STDOUT: {result.stdout}")
        print(f"STDERR: {result.stderr}")
        print(f"Exception: {result.exception}")
    
    assert result.exit_code == 0
```

### Interactive Debugging
```python
import pdb

def test_with_debugger(runner):
    """Test with interactive debugger."""
    result = runner.invoke(app, ["start"])
    
    if result.exit_code != 0:
        pdb.set_trace()  # Drop into debugger
    
    assert result.exit_code == 0
```

## Common Issues and Solutions

### 1. Environment Variables
```python
@patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key'})
def test_with_env_var(runner):
    """Test with environment variable set."""
    result = runner.invoke(app, ["start"])
    assert result.exit_code == 0
```

### 2. File Permissions
```python
def test_file_permissions(runner, tmp_path):
    """Test file permission handling."""
    read_only_file = tmp_path / "readonly.txt"
    read_only_file.write_text("content")
    read_only_file.chmod(0o444)  # Read-only
    
    result = runner.invoke(app, ["process", str(read_only_file)])
    assert result.exit_code != 0
```

### 3. Network Timeouts
```python
@patch('requests.get', side_effect=requests.exceptions.Timeout)
def test_network_timeout(mock_get, runner):
    """Test network timeout handling."""
    result = runner.invoke(app, ["fetch", "http://example.com"])
    assert result.exit_code != 0
    assert "timeout" in result.stderr.lower()
```

## Summary

- Write comprehensive unit and integration tests
- Use temporary directories for file operations
- Mock external dependencies
- Test both success and failure scenarios
- Maintain high test coverage
- Use descriptive test names and documentation
- Include performance and memory usage tests
- Set up proper CI/CD pipeline

* [ ] CI matrix: {linux, mac, windows} × {3.10–3.12}
* [ ] Coverage gate (e.g., 90%) and a smoke/perf test

If you want, I’ll turn this into a **`tests/README.md` + `noxfile.py`** and drop in working examples wired to Lily’s Typer app and prompt\_toolkit shell.
